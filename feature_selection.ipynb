{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#done\n",
    "def convert_to_float(not_float):\n",
    "    float_list = []\n",
    "    for i in not_float:\n",
    "            float_list.append([float(j) for j in i])\n",
    "    return float_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: split  test with training set\n",
    "def process_file(raw_file_name):\n",
    "    raw_file = open(raw_file_name, 'r')\n",
    "    str_data = []\n",
    "    with open(raw_file_name) as file:\n",
    "        raw_data = file.readlines()\n",
    "    str_data = [i.split() for i in raw_data]\n",
    "    return convert_to_float(str_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def take_out_classification(data):\n",
    "    classfication = []\n",
    "    for i in data:\n",
    "        classfication.append(i.pop(0))\n",
    "    return classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def add_back_classification(data, list_of_classfication):\n",
    "    for i in range(0, len(data)):\n",
    "        data[i].insert(0, float(list_of_classfication[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def getmean(data, features, instances):\n",
    "    mean_list = []\n",
    "    column = 0\n",
    "    #classif = take_out_classification(data)\n",
    "    for i in range(features):\n",
    "        mean = sum(row[column] for row in data)/instances\n",
    "        column = column + 1\n",
    "        mean_list.append(mean)\n",
    "    #add_back_classification(data, classif)\n",
    "    return mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "import math\n",
    "def getstd(data, mean_list, features, instances):\n",
    "    standard_d_list = []\n",
    "    #classif = take_out_classification(data)\n",
    "    mean_list = getmean(data, features, instances)\n",
    "    for i in range(features):\n",
    "        standard_d = math.sqrt((sum(pow((row[i] - mean_list[i]), 2) for row in data)) / instances)\n",
    "        standard_d_list.append(standard_d)\n",
    "    #add_back_classification(data, classif)\n",
    "    #df = pd.DataFrame(standard_d_list)\n",
    "    #print(df)\n",
    "    return standard_d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def scale(data):\n",
    "    classif = take_out_classification(data)\n",
    "    features = len(data[0])\n",
    "    instances = len(data)\n",
    "    #print(instances)\n",
    "    mean_list = getmean(data, features , instances)\n",
    "    standard_d_list = getstd(data, mean_list, features, instances )\n",
    "    #df  = pd.DataFrame(data)\n",
    "    #print(df)\n",
    "    \n",
    "    for i in range(0, instances):\n",
    "        for j in range(features):\n",
    "            #print(\"in loop\")\n",
    "            #print(data[i][j])\n",
    "            data[i][j] = (data[i][j] - mean_list[j])/ standard_d_list[j]\n",
    "    add_back_classification(data, classif)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0         1         2         3         4         5         6         7   \\\n",
      "0  2.0  2.862132  3.834596  3.851024  2.535483  1.974341  2.466221  1.095809   \n",
      "1  1.0  2.078912  3.384557  4.088998  3.658901  3.336261  4.938376  1.988108   \n",
      "2  2.0  2.204019  1.271734  3.189046  2.933074  3.520601  2.542561  2.263089   \n",
      "3  1.0  2.204019  1.271734  3.189046  2.933074  3.520601  2.542561  2.263089   \n",
      "4  2.0  3.217470  4.141943  1.686134  3.002474  4.163254  2.021311  3.750022   \n",
      "\n",
      "         8         9         10  \n",
      "0  2.180104  2.633061  3.148389  \n",
      "1  3.600924  3.120524  1.396510  \n",
      "2  3.634898  3.501847  4.183576  \n",
      "3  3.634898  3.501847  4.183576  \n",
      "4  2.461549  2.410751  2.338304  \n",
      "    0         1         2         3         4         5         6         7   \\\n",
      "0  2.0  0.780949  0.839207  0.776422 -1.315122 -1.842117 -0.420668 -1.376895   \n",
      "1  1.0 -0.972539  0.480773  1.060605  1.781452  0.046098  1.964636 -0.332355   \n",
      "2  2.0 -0.692449 -1.201987 -0.014096 -0.219208  0.301674 -0.347010 -0.010459   \n",
      "3  1.0 -0.692449 -1.201987 -0.014096 -0.219208  0.301674 -0.347010 -0.010459   \n",
      "4  2.0  1.576488  1.083994 -1.808836 -0.027914  1.192671 -0.849947  1.730168   \n",
      "\n",
      "         8         9         10  \n",
      "0 -1.431124 -0.898147  0.091127  \n",
      "1  0.773379  0.194897 -1.532625  \n",
      "2  0.826092  1.049942  1.050604  \n",
      "3  0.826092  1.049942  1.050604  \n",
      "4 -0.994441 -1.396635 -0.659711  \n"
     ]
    }
   ],
   "source": [
    "trainSet = [[2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00], [1.0000000e+00  ,2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00], [ 2.0000000e+00, 2.2040188e+00,   1.2717340e+00,   3.1890461e+00,   2.9330737e+00,   3.5206010e+00,   2.5425605e+00,   2.2630886e+00,   3.6348978e+00,   3.5018469e+00,   4.1835761e+00], [1.0000000e+00, 2.2040188e+00, 1.2717340e+00, 3.1890461e+00, 2.9330737e+00, 3.5206010e+00, 2.5425605e+00, 2.2630886e+00, 3.6348978e+00, 3.5018469e+00, 4.1835761e+00], [2.0000000e+00  , 3.2174703e+00  , 4.1419433e+00  , 1.6861336e+00 ,  3.0024740e+00  , 4.1632536e+00  , 2.0213114e+00  , 3.7500223e+00  , 2.4615493e+00 ,  2.4107511e+00 ,2.3383039e+00]]\n",
    "\n",
    "df  = pd.DataFrame(trainSet)\n",
    "print(df)\n",
    "trainSet = scale(trainSet)\n",
    "df  = pd.DataFrame(trainSet)\n",
    "print(df)\n",
    "#print(len(trainSet[0]) - 1)\n",
    "#print(len(trainSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me \n",
      "[[2.0, 0.9999999999999993, 1.0, 0.9999999999999993, -0.9999999999999989, -0.9999999999999999, -1.0, -1.0, -1.0, -0.9999999999999994, -1.0000000000000004], [1.0, -1.0000000000000007, -1.0, -1.0000000000000007, 1.000000000000001, 1.0000000000000002, 1.0, 1.0, 1.0, 1.0000000000000004, 0.9999999999999996]]\n"
     ]
    }
   ],
   "source": [
    "data1 = [[ 2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00],  [1.0000000e+00, 2.2040188e+00,   1.2717340e+00,   3.1890461e+00,   2.9330737e+00,   3.5206010e+00,   2.5425605e+00,   2.2630886e+00,   3.6348978e+00,   3.5018469e+00,   4.1835761e+00]]\n",
    "\n",
    "print(\"me \")\n",
    "#print(data1)\n",
    "#print(getmean(data1, 2, 2))\n",
    "#print(getstd(data1,getmean(data1, 2, 2),2, 2))\n",
    "print(scale(data1))\n",
    "#print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#done\n",
    "def euclideanDistance(instance_a, instance_b, features):\n",
    "    #remove classfication for calculation\n",
    "    #print(instance_a)\n",
    "    classification_1 = instance_a.pop(0)\n",
    "    classification_2 = instance_b.pop(0)\n",
    "    \n",
    "    distance = 0\n",
    "    #print(repr(instance_a) +\"\\n\"+ repr(instance_b))\n",
    "    for i in range(len(features)):\n",
    "        #print(instance_b)\n",
    "        #print(features[i])\n",
    "        #print(instance_b[features[i]])\n",
    "       # print(\"instance a = \" +  repr(instance_a[i]) + \" and instance b = \" +  repr(instance_b[i]))\n",
    "        distance = distance + pow((instance_a[features[i]] - instance_b[features[i]]), 2)\n",
    "       # print(\"and the distance is: \" + repr(distance) + \"\\n\")\n",
    "    \n",
    "    #add classification back\n",
    "    instance_a.insert(0, classification_1)\n",
    "    instance_b.insert(0, classification_2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 0.7832192999999998\n"
     ]
    }
   ],
   "source": [
    "data1 = [ 2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00]\n",
    "data2 = [2.0000000e+00, 2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00]\n",
    "distance = euclideanDistance(data1, data2, [0])\n",
    "print( 'Distance: ' + repr(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbor(data, test, features):\n",
    "    best_neighbor = [-1, -1]\n",
    "    distance = float('inf')\n",
    "    test_distance = float('inf')\n",
    "    for i in data:\n",
    "        if len(features) > 0:\n",
    "            test_distance = euclideanDistance(i, test, features)\n",
    "        if test_distance < distance:\n",
    "            distance = test_distance\n",
    "            #print(i[0])\n",
    "            best_neighbor[0] = i[0]\n",
    "            #print(test_distance)\n",
    "            best_neighbor[1]= test_distance\n",
    "    return best_neighbor\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1]\n"
     ]
    }
   ],
   "source": [
    "trainSet = [[2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00], [1.0000000e+00  ,2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00]]\n",
    "testInstance = [1.0000000e+00  ,2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00]\n",
    "k = 1\n",
    "neighbor = nearest_neighbor(trainSet, testInstance, [])\n",
    "print(neighbor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_accuracy(data,feature_set, feature):\n",
    "\n",
    "    #print(feature)\n",
    "    list_feature = list(feature_set)\n",
    "    accuracy = 0\n",
    "    test = []\n",
    "    neighbor = []\n",
    "    if feature < 0:\n",
    "        feature_set.discard(abs(feature))\n",
    "        list_feature = list(feature_set)\n",
    "        feature_set.add(abs(feature))\n",
    "    elif feature > 0:\n",
    "        feature_set.add(abs(feature))\n",
    "        list_feature = list(feature_set)\n",
    "        feature_set.discard(abs(feature))\n",
    "    #random(data)\n",
    "    for i in range(660):\n",
    "        np.random.shuffle(data)\n",
    "        #print(data)\n",
    "        test = data.pop(0)\n",
    "        #print(\"test \" + repr(test[0]))\n",
    "        neighbor = nearest_neighbor(data, test, list_feature)\n",
    "        #print(\"Neighbor \" + repr(neighbor))\n",
    "        #print(\"found \" + repr(nearest_neighbor(data, test, feature)[0]))\n",
    "        if  neighbor[0] == test[0]:\n",
    "            accuracy = accuracy + 1\n",
    "        data.append(test)\n",
    "    accuracy = accuracy/661\n",
    "    return accuracy * 100\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_accuracy() missing 1 required positional argument: 'feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-0ab405f1cc15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrainSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2.0000000e+00\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m2.8621318e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.8345958e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.8510244e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.5354832e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m1.9743413e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.4662208e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m1.0958087e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.1801037e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.6330608e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.1483889e+00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0000000e+00\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[1;36m2.0789125e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.3845572e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m4.0889981e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m3.6589007e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m3.3362607e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m4.9383758e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m1.9881081e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m3.6009237e+00\u001b[0m   \u001b[1;33m,\u001b[0m\u001b[1;36m3.1205240e+00\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[1;36m1.3965097e+00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;36m2.0000000e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.2040188e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m1.2717340e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.1890461e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.9330737e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.5206010e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.5425605e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m2.2630886e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.6348978e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m3.5018469e+00\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[1;36m4.1835761e+00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0000000e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.2040188e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2717340e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.1890461e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.9330737e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.5206010e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.5425605e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.2630886e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.6348978e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.5018469e+00\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.1835761e+00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2.0000000e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m3.2174703e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m4.1419433e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m1.6861336e+00\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[1;36m3.0024740e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m4.1632536e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m2.0213114e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m3.7500223e+00\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[1;36m2.4615493e+00\u001b[0m \u001b[1;33m,\u001b[0m  \u001b[1;36m2.4107511e+00\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m2.3383039e+00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_accuracy() missing 1 required positional argument: 'feature'"
     ]
    }
   ],
   "source": [
    "trainSet = [[2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00], [1.0000000e+00  ,2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00], [ 2.0000000e+00, 2.2040188e+00,   1.2717340e+00,   3.1890461e+00,   2.9330737e+00,   3.5206010e+00,   2.5425605e+00,   2.2630886e+00,   3.6348978e+00,   3.5018469e+00,   4.1835761e+00], [1.0000000e+00, 2.2040188e+00, 1.2717340e+00, 3.1890461e+00, 2.9330737e+00, 3.5206010e+00, 2.5425605e+00, 2.2630886e+00, 3.6348978e+00, 3.5018469e+00, 4.1835761e+00], [2.0000000e+00  , 3.2174703e+00  , 4.1419433e+00  , 1.6861336e+00 ,  3.0024740e+00  , 4.1632536e+00  , 2.0213114e+00  , 3.7500223e+00  , 2.4615493e+00 ,  2.4107511e+00 ,2.3383039e+00]]\n",
    "accuracy = 0\n",
    "accuracy = compute_accuracy(trainSet, [])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(data):\n",
    "    print(\"Froward selection\")\n",
    "    features = len(data[0]) - 1\n",
    "    feature_set = set()\n",
    "    best_accuracy = 0\n",
    "    #print(best_accuracy)\n",
    "    for i in range(features):\n",
    "        print(\"At level: \" + repr(i+1))\n",
    "        add_this = -1\n",
    "        for j in range(i, features):\n",
    "            if j not in feature_set:\n",
    "                accuracy = compute_accuracy(data, feature_set, j)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    add_this = j\n",
    "        if add_this > 0:\n",
    "            feature_set.add(add_this)\n",
    "            print(\"Best accuracy so far: \" + \"%.2f\" % best_accuracy + \" %\" + \" With feature set: \" + repr(feature_set))\n",
    "        else:\n",
    "            print(\"(Warning, Accuracy has decreased! Continuing search in case of local maxima)\")\n",
    "    print(\"This is what I got: \")\n",
    "    print(\"With feature set: \" + repr(feature_set)+ \" Best accuracy: \" + \"%.2f\" % best_accuracy + \" %\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froward selection\n",
      "At level: 1\n",
      "Best accuracy so far: 40.24 % With feature set: {1}\n",
      "At level: 2\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 3\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 4\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "This is what I got: \n",
      "With feature set: {1} Best accuracy: 40.24 %\n"
     ]
    }
   ],
   "source": [
    "data = [[2.0000000e+00 ,2.8621318e+00,   3.8345958e+00,   3.8510244e+00,   2.5354832e+00,   1.9743413e+00,   2.4662208e+00,   1.0958087e+00,   2.1801037e+00,   2.6330608e+00,   3.1483889e+00], [1.0000000e+00  ,2.0789125e+00, 3.3845572e+00   ,4.0889981e+00   ,3.6589007e+00   ,3.3362607e+00   ,4.9383758e+00   ,1.9881081e+00   ,3.6009237e+00   ,3.1205240e+00  ,1.3965097e+00], [ 2.0000000e+00, 2.2040188e+00,   1.2717340e+00,   3.1890461e+00,   2.9330737e+00,   3.5206010e+00,   2.5425605e+00,   2.2630886e+00,   3.6348978e+00,   3.5018469e+00,   4.1835761e+00], [1.0000000e+00, 2.2040188e+00, 1.2717340e+00, 3.1890461e+00, 2.9330737e+00, 3.5206010e+00, 2.5425605e+00, 2.2630886e+00, 3.6348978e+00, 3.5018469e+00, 4.1835761e+00], [2.0000000e+00  , 3.2174703e+00  , 4.1419433e+00  , 1.6861336e+00 ,  3.0024740e+00  , 4.1632536e+00  , 2.0213114e+00  , 3.7500223e+00  , 2.4615493e+00 ,  2.4107511e+00 ,2.3383039e+00]]\n",
    "\n",
    "\n",
    "trainSet = [[2.0000000e+00, 2.8621318e+00, 3.8345958e+00, 3.8510244e+00, 2.5354832e+00],\n",
    "            [1.0000000e+00, 2.0789125e+00, 3.3845572e+00, 4.0889981e+00, 3.6589007e+00], \n",
    "            [2.0000000e+00, 2.2040188e+00, 1.2717340e+00, 3.1890461e+00, 2.9330737e+00], \n",
    "            [1.0000000e+00, 2.2040188e+00, 1.2717340e+00, 3.1890461e+00, 2.9330737e+00],\n",
    "            [2.0000000e+00, 3.2174703e+00, 4.1419433e+00, 1.6861336e+00, 3.0024740e+00]]\n",
    "\n",
    "#df  = pd.DataFrame(trainSet)\n",
    "#print(df)\n",
    "trainSet = scale(trainSet)\n",
    "#print(trainSet)\n",
    "\n",
    "#df  = pd.DataFrame(trainSet)\n",
    "#print(df)\n",
    "\n",
    "forward_selection(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection(data):\n",
    "    print(\"Backward selection\")\n",
    "    features = len(data[0]) - 1\n",
    "    feature_set = set(i for i in range(features))\n",
    "    #print(feature_set)\n",
    "    best_accuracy = 0\n",
    "    #print(best_accuracy)\n",
    "    for i in range(features):\n",
    "        print(\"At level: \" + repr(i+1))\n",
    "        remove_this = -1\n",
    "        for j in range(features):\n",
    "            if j in feature_set:\n",
    "                accuracy = compute_accuracy(data, feature_set, -1*j)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    remove_this = j\n",
    "        if remove_this > 0:\n",
    "            feature_set.discard(remove_this)\n",
    "            print(\"Best accuracy so far: \" + \"%.2f\" % best_accuracy + \" %\" + \" With feature set: \" + repr(feature_set))\n",
    "        else:\n",
    "            print(\"(Warning, Accuracy has decreased! Continuing search in case of local maxima)\")\n",
    "    print(\"This is what I got: \")\n",
    "    print(\"With feature set: \" + repr(feature_set)+ \" Best accuracy: \" + \"%.2f\" % best_accuracy + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_algorithm(data):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froward selection\n",
      "At level: 1\n",
      "Best accuracy so far: 83.06 % With feature set: {4}\n",
      "At level: 2\n",
      "Best accuracy so far: 88.05 % With feature set: {4, 8}\n",
      "At level: 3\n",
      "Best accuracy so far: 91.38 % With feature set: {3, 4, 8}\n",
      "At level: 4\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 5\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 6\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 7\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 8\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 9\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 10\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "This is what I got: \n",
      "With feature set: {3, 4, 8} Best accuracy: 91.38 %\n",
      "Backward selection\n",
      "At level: 1\n",
      "Best accuracy so far: 74.43 % With feature set: {0, 1, 2, 3, 4, 5, 7, 8, 9}\n",
      "At level: 2\n",
      "Best accuracy so far: 77.00 % With feature set: {0, 1, 2, 3, 4, 7, 8, 9}\n",
      "At level: 3\n",
      "Best accuracy so far: 77.31 % With feature set: {0, 1, 2, 3, 4, 8, 9}\n",
      "At level: 4\n",
      "Best accuracy so far: 85.02 % With feature set: {0, 1, 2, 4, 8, 9}\n",
      "At level: 5\n",
      "Best accuracy so far: 86.38 % With feature set: {0, 1, 2, 4, 9}\n",
      "At level: 6\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 7\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 8\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 9\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "At level: 10\n",
      "(Warning, Accuracy has decreased! Continuing search in case of local maxima)\n",
      "This is what I got: \n",
      "With feature set: {0, 1, 2, 4, 9} Best accuracy: 88.05 %\n"
     ]
    }
   ],
   "source": [
    "raw_file_open = 'CS170Smalltestdata__37.txt'\n",
    "data = process_file(raw_file_open)\n",
    "scaled_data = scale(data)\n",
    "\n",
    "forward_selection(scaled_data)\n",
    "backward_selection(scaled_data)\n",
    "\n",
    "#nearest_neighbor(data, test, )\n",
    "#print(data)\n",
    "#print(data[0][1])\n",
    "#print(data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
